{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorgealarconlucas/TFG/blob/master/Modelos_de_machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statistics as stats\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import scipy.stats as ss\n",
        "import statistics as stats\n",
        "import seaborn as sns \n",
        "from pandas.plotting import autocorrelation_plot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import export_graphviz\n",
        "from six import StringIO\n",
        "from IPython.display import Image  \n",
        "import pydotplus\n",
        "from sklearn.model_selection import  GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score,recall_score, precision_score, confusion_matrix\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "-ruNeqFp9jnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = pd.read_feather('x.feather')\n",
        "y_train = pd.read_feather('y.feather')"
      ],
      "metadata": {
        "id": "vxi0qdBF9isu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning para revision = 1"
      ],
      "metadata": {
        "id": "mXcVUQjKBeKj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2vhEerI65bC"
      },
      "outputs": [],
      "source": [
        "x_train_1 = X_train[X_train['Revision'] == 1]\n",
        "\n",
        "x_train_1.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US2sPygL62WC"
      },
      "outputs": [],
      "source": [
        "len(y_train)\n",
        "\n",
        "#get a boolean np.array as index\n",
        "idx = X_train['Revision'] == 1\n",
        "\n",
        "idx = idx.values\n",
        "\n",
        "y_train_aux = y_train.values\n",
        "\n",
        "y_train_1 = y_train_aux[idx]\n",
        "\n",
        "print(y_train_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "8iA8M8HS-O_s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2_52kUXC7tS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer, load_diabetes, load_wine\n",
        "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimización de los hiperparámetros usando RandomizedSearchCV"
      ],
      "metadata": {
        "id": "b0BmHhZ0Li2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Indico que el clasificador que voy a utlizar es XGBosst\n",
        "\n",
        "xgb_model = xgb.XGBClassifier()"
      ],
      "metadata": {
        "id": "BHAN5Yve-9Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicialmente creamos un diccionario de algunos parámetros a entrenar. Aquí las claves son básicamente los parámetros y los valores a entrenar. Así que el RandomizedSearchCV probará cada valor y encontrará el valor particular que da la mayor precisión."
      ],
      "metadata": {
        "id": "lCpdvaAR-Nqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cuanto mayor sea gamma, más conservador será el algoritmo\n",
        "#min_child_weight: para controlar el sobreajuste, cuanto mayor sea min_child_weight, más conservador será el algoritmo\n",
        "\n",
        "params = {\n",
        " 'learning_rate' : [0.05,0.10,0.15,0.20,0.25,0.30],\n",
        " 'max_depth' : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
        " 'min_child_weight' : [ 1, 3, 5, 7 ],\n",
        " 'gamma': [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
        " 'colsample_bytree' : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
        "}"
      ],
      "metadata": {
        "id": "Klmci5LdABp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, llamamos a RandomizedSearchCV() y le pasamos los siguientes parámetros"
      ],
      "metadata": {
        "id": "Z_8hIRiEAqFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#verbose = genera mensajes durante el entramiento del modelo 'Fitting 5 folds...'\n",
        "\n",
        "#roc_auc = curva AUC-ROC es la métrica de selección del modelo para el problema de clasificación \n",
        "#de dos clases múltiples.ROC nos dice qué tan bueno es el modelo para distinguir las clases dadas, \n",
        "#en términos de la probabilidad predicha.\n",
        "\n",
        "#n_jobs = número de nucleos que se utilizan (-1 quiere decir que se utilizan todos)\n",
        "\n",
        "r_s_model = RandomizedSearchCV(xgb_model , param_distributions=params, n_iter=5, \n",
        "                               scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)\n",
        "r_s_model.fit(x_train_1, y_train_1)"
      ],
      "metadata": {
        "id": "3SHHuaOcAvQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bien, nuestro modelo ha sido ajustado. Veamos ahora todos los parámetros que han sido seleccionados por el RandomizedSearch() para el XGBClassifier. Podemos hacerlo con la ayuda del método best_estimators_."
      ],
      "metadata": {
        "id": "PwHr_OxHCJBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r_s_model.best_estimator_"
      ],
      "metadata": {
        "id": "rXz112xuCL9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, como conocemos todos los mejores parámetros, podemos simplemente construir nuestro modelo clasificador final pasando todos esos parámetros."
      ],
      "metadata": {
        "id": "xG-bVWyjCZcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Construyendo el modelo final\n",
        "xgb_model = xgb.XGBClassifier(colsample_bytree=0.7, gamma=0.3, learning_rate=0.3, max_depth=8)"
      ],
      "metadata": {
        "id": "MF6gOB1RChQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Métricas\n",
        "\n",
        "xgb_model.fit(x_train_1, y_train_1)\n",
        "\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "acc_xgb = accuracy_score(y_test, y_pred)\n",
        "sensibilidad_xgb = recall_score(y_test, y_pred)\n",
        "#precision_xgb = precision_score(y_test, y_pred)\n",
        "specificity_xgb = confusion_matrix(y_test, y_pred)[0][0]/(confusion_matrix(y_test, y_pred)[0][0]+confusion_matrix(y_test, y_pred)[0][1])\n",
        "auc_xgb = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "Tabla = pd.DataFrame({ \"Prestaciones en test\":[\"Accuracy\",\"Sensibility\",'Specificity',\"AUC ROC\"],\n",
        "                      \"XGBoost\" : [acc_xgb, sensibilidad_xgb, specificity_xgb, auc_xgb]})\n",
        "\n",
        "Tabla"
      ],
      "metadata": {
        "id": "NWmveSOUG3Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selección de características (FS)"
      ],
      "metadata": {
        "id": "1_w29Eajzaja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La selección de características se entiende como una disminución del número de éstas, en función de un criterio elegido por el usuario, escogiendo aquellas que se consideran más informativas y eliminando aquellas que son irrelevantes. "
      ],
      "metadata": {
        "id": "PL0SquGRzibQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importancia de cada característica**"
      ],
      "metadata": {
        "id": "mp9Vyp9DzlTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La importancia de características desempeña un papel importante, ya que proporciona una visión de los datos, del modelo y la base para la reducción de la dimensionalidad y la selección de características que pueden mejorar la eficiencia y la eficacia de un modelo predictivo en el problema."
      ],
      "metadata": {
        "id": "U4qQSKeWzmFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "#Plot feature importance\n",
        "def plot_feature_importance(importance,names,model_type):\n",
        "\n",
        "  #Create arrays from feature importance and feature names\n",
        "  feature_importance = np.array(importance)\n",
        "  feature_names = np.array(names)\n",
        "\n",
        "  #Create a DataFrame using a Dictionary\n",
        "  data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
        "  fi_df = pd.DataFrame(data)\n",
        "\n",
        "  #Sort the DataFrame in order decreasing feature importance\n",
        "  fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
        "\n",
        "  #Define size of bar plot\n",
        "  fig = plt.figure(figsize=(20,6))\n",
        "  #Plot Searborn bar chart\n",
        "  sns.barplot(y=fi_df['feature_importance'], x=fi_df['feature_names'])\n",
        "  #Add chart labels\n",
        "  plt.title(model_type + 'FEATURE IMPORTANCE')\n",
        "  plt.xlabel('FEATURE IMPORTANCE')\n",
        "  plt.ylabel('FEATURE NAMES')\n",
        "  plt.tight_layout()\n",
        "  fig.autofmt_xdate(rotation=45)\n",
        "\n",
        "\n",
        "plot_feature_importance(xgb_model.feature_importances_*100,x_train_1.columns,'RANDOM FOREST ')\n",
        "\n",
        "print(np.sum(xgb_model.feature_importances_))"
      ],
      "metadata": {
        "id": "j2lDeTU6zZ47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selección de las características en base a la importancia de éstas**"
      ],
      "metadata": {
        "id": "Lw5m7_pB2id9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SelectFromModel\n",
        "\n",
        "scores_cv = []\n",
        "thresholds = np.sort(xgb_model.feature_importances_) # obtiene la importancia de cada característica\n",
        "i = 0\n",
        "for thresh in thresholds:\n",
        "    print(\"{} de {}\".format(i,len(thresholds)))\n",
        "    selection = SelectFromModel(xgb_model, threshold=thresh, prefit=True)\n",
        "    select_x_train_1 = selection.transform(x_train_1)\n",
        "    # train model\n",
        "    selection_model = xgb.XGBClassifier(colsample_bytree=0.7, gamma=0.3, learning_rate=0.3, max_depth=8)\n",
        "    #cv estimation\n",
        "    sc =cross_val_score(selection_model, select_x_train_1, y_train_1, cv=10, scoring='roc_auc',n_jobs=-1)\n",
        "    scores_cv.append(sc)\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "0iM8ZAFW0kJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rc('xtick',labelsize=15)\n",
        "plt.rc('ytick',labelsize=15)\n",
        "plt.figure(figsize = (80,20))\n",
        "sc_cv = np.asarray(scores_cv)\n",
        "scores_m = np.mean(sc_cv,axis = 1)\n",
        "scores_std = np.std(sc_cv,axis = 1)\n",
        "xx = np.arange(len(scores_m))\n",
        "plt.plot(xx,scores_m,'o-')\n",
        "plt.grid()\n",
        "_ = plt.xticks(xx,labels= np.arange(1,len(scores_m)+1))\n",
        "plt.xlabel('Number of features removed')\n",
        "plt.ylabel('Accuracy')"
      ],
      "metadata": {
        "id": "dZpG_Bv92uN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8MaaLxwrHnY"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK45kpe0BT4W"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_hastie_10_2\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Indico que el clasificador que voy a utlizar es GBoosting\n",
        "\n",
        "gb_model = GradientBoostingClassifier()"
      ],
      "metadata": {
        "id": "70RPb-H7MWNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        " 'n_estimators' : [25,50,75,100,125,150,175,200,250,300,350,400],\n",
        " 'learning_rate' : [0.05,0.10,0.15,0.20,0.25,0.30],\n",
        " 'max_depth' : [ 3, 4, 5, 6, 8, 10, 12, 15,20,25,30],\n",
        "}"
      ],
      "metadata": {
        "id": "aMXjUhN5Mgdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verbose = genera mensajes durante el entramiento del modelo 'Fitting 5 folds...'\n",
        "\n",
        "#roc_auc = curva AUC-ROC es la métrica de selección del modelo para el problema de clasificación \n",
        "#de dos clases múltiples.ROC nos dice qué tan bueno es el modelo para distinguir las clases dadas, \n",
        "#en términos de la probabilidad predicha.\n",
        "\n",
        "#n_jobs = número de nucleos que se utilizan (-1 quiere decir que se utilizan todos)\n",
        "\n",
        "r_s_model_2 = RandomizedSearchCV(gb_model , param_distributions=params, n_iter=5, \n",
        "                               scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)\n",
        "\n",
        "r_s_model_2.fit(x_train_1, y_train_1)"
      ],
      "metadata": {
        "id": "Ap81OEvyMjTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r_s_model_2.best_estimator_"
      ],
      "metadata": {
        "id": "1ohuKN0wMqqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Construyendo el modelo final\n",
        "gb_model = GradientBoostingClassifier(learning_rate=0.05, max_depth=25, n_estimators=350)"
      ],
      "metadata": {
        "id": "22LZf3hgMvey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Métricas\n",
        "\n",
        "gb_model.fit(x_train_1, y_train_1)\n",
        "\n",
        "y_pred = gb_model.predict(X_test)\n",
        "\n",
        "acc_gb = accuracy_score(y_test, y_pred)\n",
        "sensibilidad_gb = recall_score(y_test, y_pred)\n",
        "#precision_xgb = precision_score(y_test, y_pred)\n",
        "specificity_gb = confusion_matrix(y_test, y_pred)[0][0]/(confusion_matrix(y_test, y_pred)[0][0]+confusion_matrix(y_test, y_pred)[0][1])\n",
        "auc_gb = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "Tabla_2 = pd.DataFrame({ \"Prestaciones en test\":[\"Accuracy\",\"Sensibility\",'Specificity',\"AUC ROC\"],\n",
        "                      \"XGBoost\" : [acc_gb, sensibilidad_gb, specificity_gb, auc_gb]})\n",
        "\n",
        "Tabla_2"
      ],
      "metadata": {
        "id": "53iLdlm0M4hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selección de características (FS)"
      ],
      "metadata": {
        "id": "zdwcnUt24xrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "#Plot feature importance\n",
        "def plot_feature_importance(importance,names,model_type):\n",
        "\n",
        "  #Create arrays from feature importance and feature names\n",
        "  feature_importance = np.array(importance)\n",
        "  feature_names = np.array(names)\n",
        "\n",
        "  #Create a DataFrame using a Dictionary\n",
        "  data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
        "  fi_df = pd.DataFrame(data)\n",
        "\n",
        "  #Sort the DataFrame in order decreasing feature importance\n",
        "  fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
        "\n",
        "  #Define size of bar plot\n",
        "  fig = plt.figure(figsize=(20,6))\n",
        "  #Plot Searborn bar chart\n",
        "  sns.barplot(y=fi_df['feature_importance'], x=fi_df['feature_names'])\n",
        "  #Add chart labels\n",
        "  plt.title(model_type + 'FEATURE IMPORTANCE')\n",
        "  plt.xlabel('FEATURE IMPORTANCE')\n",
        "  plt.ylabel('FEATURE NAMES')\n",
        "  plt.tight_layout()\n",
        "  fig.autofmt_xdate(rotation=45)\n",
        "\n",
        "\n",
        "plot_feature_importance(gb_model.feature_importances_*100,x_train_1.columns,'RANDOM FOREST ')\n",
        "\n",
        "print(np.sum(gb_model.feature_importances_))"
      ],
      "metadata": {
        "id": "yQl1rMUr4xGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selección de las características en base a la importancia de éstas**"
      ],
      "metadata": {
        "id": "slKvOlOp48LL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SelectFromModel\n",
        "\n",
        "scores_cv = []\n",
        "thresholds = np.sort(gb_model.feature_importances_) # obtiene la importancia de cada característica\n",
        "i = 0\n",
        "for thresh in thresholds:\n",
        "    print(\"{} de {}\".format(i,len(thresholds)))\n",
        "    selection = SelectFromModel(gb_model, threshold=thresh, prefit=True)\n",
        "    select_x_train_1 = selection.transform(x_train_1)\n",
        "    # train model\n",
        "    selection_model = GradientBoostingClassifier(learning_rate=0.05, max_depth=25, n_estimators=350)\n",
        "    #cv estimation\n",
        "    sc =cross_val_score(selection_model, select_x_train_1, y_train_1, cv=10, scoring='roc_auc',n_jobs=-1)\n",
        "    scores_cv.append(sc)\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "9OdM9pcr48_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rc('xtick',labelsize=15)\n",
        "plt.rc('ytick',labelsize=15)\n",
        "plt.figure(figsize = (80,20))\n",
        "sc_cv = np.asarray(scores_cv)\n",
        "scores_m = np.mean(sc_cv,axis = 1)\n",
        "scores_std = np.std(sc_cv,axis = 1)\n",
        "xx = np.arange(len(scores_m))\n",
        "plt.plot(xx,scores_m,'o-')\n",
        "plt.grid()\n",
        "_ = plt.xticks(xx,labels= np.arange(1,len(scores_m)+1))\n",
        "plt.xlabel('Number of features removed')\n",
        "plt.ylabel('Accuracy')"
      ],
      "metadata": {
        "id": "JehCEQKh5MEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI-esAG5rP9Q"
      },
      "source": [
        "## Histogram-based Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.datasets import load_iris"
      ],
      "metadata": {
        "id": "Jtl4ZFN3bbe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Indico que el clasificador que voy a utlizar es XGBosst\n",
        "\n",
        "hb_model = HistGradientBoostingClassifier()"
      ],
      "metadata": {
        "id": "FQ2c4qzsbb8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        " 'max_leaf_nodes': [1,3,5,7,10,13,15,20,25,30,35,40],\n",
        " 'learning_rate' : [0.05,0.10,0.15,0.20,0.25,0.30],\n",
        " 'max_depth' : [ 3, 4, 5, 6, 8, 10, 12, 15,20,25,30],\n",
        "}"
      ],
      "metadata": {
        "id": "ymoVMrSmbcLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verbose = genera mensajes durante el entramiento del modelo 'Fitting 5 folds...'\n",
        "\n",
        "#roc_auc = curva AUC-ROC es la métrica de selección del modelo para el problema de clasificación \n",
        "#de dos clases múltiples.ROC nos dice qué tan bueno es el modelo para distinguir las clases dadas, \n",
        "#en términos de la probabilidad predicha.\n",
        "\n",
        "#n_jobs = número de nucleos que se utilizan (-1 quiere decir que se utilizan todos)\n",
        "\n",
        "r_s_model_3 = RandomizedSearchCV(hb_model , param_distributions=params, n_iter=5, \n",
        "                               scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)\n",
        "r_s_model_3.fit(x_train_1, y_train_1)"
      ],
      "metadata": {
        "id": "0jOtwpuEbcTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r_s_model_3.best_estimator_"
      ],
      "metadata": {
        "id": "hj7W2xHRbix2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Construyendo el modelo final\n",
        "hb_model = HistGradientBoostingClassifier(learning_rate=0.05, max_depth=25, max_leaf_nodes=25)"
      ],
      "metadata": {
        "id": "wdCrrnJabi6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Métricas\n",
        "\n",
        "hb_model.fit(x_train_1, y_train_1)\n",
        "\n",
        "y_pred = hb_model.predict(X_test)\n",
        "\n",
        "acc_hb = accuracy_score(y_test, y_pred)\n",
        "sensibilidad_hb = recall_score(y_test, y_pred)\n",
        "#precision_xgb = precision_score(y_test, y_pred)\n",
        "specificity_hb = confusion_matrix(y_test, y_pred)[0][0]/(confusion_matrix(y_test, y_pred)[0][0]+confusion_matrix(y_test, y_pred)[0][1])\n",
        "auc_hb = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "Tabla_3 = pd.DataFrame({ \"Prestaciones en test\":[\"Accuracy\",\"Sensibility\",'Specificity',\"AUC ROC\"],\n",
        "                      \"XGBoost\" : [acc_hb, sensibilidad_hb, specificity_hb, auc_hb]})\n",
        "\n",
        "Tabla_3"
      ],
      "metadata": {
        "id": "coLW01zXdkLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selección de características (FS)"
      ],
      "metadata": {
        "id": "miIFpLHH5YoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "#Plot feature importance\n",
        "def plot_feature_importance(importance,names,model_type):\n",
        "\n",
        "  #Create arrays from feature importance and feature names\n",
        "  feature_importance = np.array(importance)\n",
        "  feature_names = np.array(names)\n",
        "\n",
        "  #Create a DataFrame using a Dictionary\n",
        "  data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
        "  fi_df = pd.DataFrame(data)\n",
        "\n",
        "  #Sort the DataFrame in order decreasing feature importance\n",
        "  fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
        "\n",
        "  #Define size of bar plot\n",
        "  fig = plt.figure(figsize=(20,6))\n",
        "  #Plot Searborn bar chart\n",
        "  sns.barplot(y=fi_df['feature_importance'], x=fi_df['feature_names'])\n",
        "  #Add chart labels\n",
        "  plt.title(model_type + 'FEATURE IMPORTANCE')\n",
        "  plt.xlabel('FEATURE IMPORTANCE')\n",
        "  plt.ylabel('FEATURE NAMES')\n",
        "  plt.tight_layout()\n",
        "  fig.autofmt_xdate(rotation=45)\n",
        "\n",
        "\n",
        "plot_feature_importance(hb_model.feature_importances_*100,x_train_1.columns,'RANDOM FOREST ')\n",
        "\n",
        "print(np.sum(hb_model.feature_importances_))"
      ],
      "metadata": {
        "id": "UF7lKwhL5Y0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selección de las características en base a la importancia de éstas**"
      ],
      "metadata": {
        "id": "Uu8VMWix5n00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SelectFromModel\n",
        "\n",
        "scores_cv = []\n",
        "thresholds = np.sort(hb_model.feature_importances_) # obtiene la importancia de cada característica\n",
        "i = 0\n",
        "for thresh in thresholds:\n",
        "    print(\"{} de {}\".format(i,len(thresholds)))\n",
        "    selection = SelectFromModel(hb_model, threshold=thresh, prefit=True)\n",
        "    select_x_train_1 = selection.transform(x_train_1)\n",
        "    # train model\n",
        "    selection_model = HistGradientBoostingClassifier(learning_rate=0.05, max_depth=25, max_leaf_nodes=25)\n",
        "    #cv estimation\n",
        "    sc =cross_val_score(selection_model, select_x_train_1, y_train_1, cv=10, scoring='roc_auc',n_jobs=-1)\n",
        "    scores_cv.append(sc)\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "00Ov1Kpq5n88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rc('xtick',labelsize=15)\n",
        "plt.rc('ytick',labelsize=15)\n",
        "plt.figure(figsize = (80,20))\n",
        "sc_cv = np.asarray(scores_cv)\n",
        "scores_m = np.mean(sc_cv,axis = 1)\n",
        "scores_std = np.std(sc_cv,axis = 1)\n",
        "xx = np.arange(len(scores_m))\n",
        "plt.plot(xx,scores_m,'o-')\n",
        "plt.grid()\n",
        "_ = plt.xticks(xx,labels= np.arange(1,len(scores_m)+1))\n",
        "plt.xlabel('Number of features removed')\n",
        "plt.ylabel('Accuracy')"
      ],
      "metadata": {
        "id": "5mXll6dW6GQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning para todas las revisiones"
      ],
      "metadata": {
        "id": "LWyGEa_1fid8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1klGt7oBBeax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZXsIk3ig90lW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Te damos la bienvenida a Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}